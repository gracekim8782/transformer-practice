{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "KN5tFHkmChcu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Implementation"
      ],
      "metadata": {
        "id": "fRdWDQt8CW2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "OinexaIKFqgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, n_layers, hidden_dim, n_heads):\n",
        "      \n",
        "        super(Transformer, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.sos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "        self.mask_idx = 0\n",
        "        self.pad_idx = 1\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding_scale = np.sqrt(self.embdding_dim)\n",
        "\n",
        "        self.fc1 = Linear(self.embedding_dim, self.output_vocab_size)\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            self.input_vocab_size,\n",
        "            self.embedding_dim,\n",
        "            self.n_layers,\n",
        "            self.hidden_dim,\n",
        "            self.n_heads,\n",
        "            self.pad_idx,\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            self.input_vocab_size,\n",
        "            self.embedding_dim,\n",
        "            self.n_layers,\n",
        "            self.hidden_dim,\n",
        "            self.n_heads,\n",
        "            self.pad_idx,\n",
        "        )\n",
        "\n",
        "    def forward(self, source, targets, source_mask=None, tgt_mask=None):\n",
        "        y = self.decoder(\n",
        "            targets, self.encoder(source, source_mask), source_mask, tgt_mask\n",
        "        )\n",
        "        y = self.fc1(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "ZAIpfeZ1Ecsw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder"
      ],
      "metadata": {
        "id": "PUuUbilYFBBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CeDyIWsFCO4T"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, n_layers, hidden_dim,\n",
        "                 n_heads, pad_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = input_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = Embedding(num_embeddings=self.vocab_size,\n",
        "                                   embedding_dim=self.embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.pos_encoding = PositionalEmbedding(self.embedding_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(\n",
        "            self.embedding_dim, hidden_dim, n_heads) for x in range(self.n_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embed(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "    def embed(self, source):\n",
        "        x = self.embedding(source)\n",
        "        positional_encoding = self.pos_encoding(x)\n",
        "        x += positional_encoding\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, n_layers, hidden_dim, n_heads, pad_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = Embedding(\n",
        "            num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.pos_encoding = PositionalEmbedding(self.embedding_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer(\n",
        "            self.embedding_dim, hidden_dim, n_heads)for x in range(self.n_layers)])\n",
        "\n",
        "    def forward(self, x, memory, source_mask=None, attention_mask=None):\n",
        "        x = self.embed(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, source_mask, attention_mask)\n",
        "        return x\n",
        "\n",
        "    def embed(self, source):\n",
        "        x = self.embeding(source)\n",
        "        positional_encoding = self.pos_encoding(x)\n",
        "        x += positional_encoding\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layers"
      ],
      "metadata": {
        "id": "h5284qdxCgOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
        "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "    return m\n",
        "\n",
        "\n",
        "def Linear(in_features, out_features, bias=True):\n",
        "    m = nn.Linear(in_features, out_features, bias)\n",
        "    return m\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_sentence_length=512):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        positional_embedding = torch.zeros(max_sentence_length, d_model)\n",
        "\n",
        "        for position in range(max_sentence_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                positional_embedding[position, i] = torch.sin(\n",
        "                    position / (10000 ** (2*i/d_model)))\n",
        "                positional_embedding[position, i +\n",
        "                                     1] = torch.cos(position / (10000 ** (2*i/d_model)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_embedding.requires_grad_(False)"
      ],
      "metadata": {
        "id": "BV2nIOhOCZKs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Layer"
      ],
      "metadata": {
        "id": "0kJNdVbxCr1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_heads):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # TO DO: Replace with custom multihead attention class\n",
        "        self.self_attention = nn.MultiheadAttention(\n",
        "            self.embedding_dim, self.n_heads)\n",
        "\n",
        "        self.fc1 = Linear(self.embedding_dim, self.hidden_dim)\n",
        "        self.fc2 = Linear(self.hidden_dim, self.embedding_dim)\n",
        "\n",
        "        self.self_attention_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def forward(self, x, source_mask=None):\n",
        "        r = x\n",
        "        x = self.self_attention_layer_norm(x)\n",
        "        x, _ = self.self_attention(\n",
        "            query=x, key=x, value=x, key_padding_mask=source_mask)\n",
        "        x = x + r\n",
        "\n",
        "        r = x\n",
        "        x = self.final_layer_norm(x)\n",
        "        # Check dimension, need to transpose?\n",
        "        x = self.fc2(self.fc1(x).relu())\n",
        "        x = x + r\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_heads):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.self_attention = nn.MultiheadAttention(\n",
        "            self.embedding_dim, self.n_heads\n",
        "        )\n",
        "        self.self_attention_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "\n",
        "        # TO DO: Replace with custom multihead attention class\n",
        "        self.enc_attention = nn.MultiheadAttention(\n",
        "            self.embedding_dim, self.n_heads\n",
        "        )\n",
        "        self.enc_attention_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "\n",
        "        self.fc1 = Linear(self.embedding_dim, self.hidden_dim)\n",
        "        self.fc2 = Linear(self.hidden_dim, self.embedding_dim)\n",
        "\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embedding_dim)\n",
        "\n",
        "    def forward(self, x, enc_out, source_mask=None, tgt_mask=None):\n",
        "        r = x\n",
        "        x = self.self_attention_layer_norm(x)\n",
        "        x, _ = self.self_attention(query=x, key=x, value=x, attn_mask=tgt_mask)\n",
        "        x = x + r\n",
        "\n",
        "        r = x\n",
        "        x = self.enc_attention_layer_norm(x)\n",
        "        x, _ = self.enc_attention(\n",
        "            query=x, key=enc_out, value=enc_out, key_padding_mask=source_mask\n",
        "        )\n",
        "        x = x + r\n",
        "\n",
        "        r = x\n",
        "        x = self.final_layer_norm(x)\n",
        "        x = self.fc2(self.fc1(x).relu())\n",
        "        x = x + r\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "i10ZjNJhCp9H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head Attention"
      ],
      "metadata": {
        "id": "GnUKtjmoFPbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)"
      ],
      "metadata": {
        "id": "paLpaTGmFOUW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ExfpGL4uFjtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import pad\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "import spacy"
      ],
      "metadata": {
        "id": "WASG1feRF0Nk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "aF4saRz2_7Ol"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_tokenizer\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "HMvcMTBSts2w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, tokenizer):\n",
        "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer, index):\n",
        "    for from_to_tuple in data_iter:\n",
        "        yield tokenizer(from_to_tuple[index])\n",
        "\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return tokenize(text, spacy_de)\n",
        "\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return tokenize(text, spacy_en)"
      ],
      "metadata": {
        "id": "fcuFZzp4wclC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = Multi30k(language_pair=(\"de\", \"en\"))"
      ],
      "metadata": {
        "id": "a1llLt-TwTbY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_src = build_vocab_from_iterator(\n",
        "    yield_tokens(train, tokenize_de, index=0),\n",
        "    min_freq=2,\n",
        "    specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
        ")\n",
        "\n",
        "vocab_tgt = build_vocab_from_iterator(\n",
        "    yield_tokens(train, tokenize_en, index=1),\n",
        "    min_freq=2,\n",
        "    specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "gPzIU8Fown4y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_map = to_map_style_dataset(train)\n",
        "val_map = to_map_style_dataset(val)"
      ],
      "metadata": {
        "id": "2d2vCNvd7w9N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(\n",
        "    batch,\n",
        "    src_tokenizer,\n",
        "    tgt_tokenizer,\n",
        "    src_vocab,\n",
        "    tgt_vocab,\n",
        "    device,\n",
        "    max_padding=128,\n",
        "    pad_id=2,\n",
        "):\n",
        "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
        "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
        "    src_list, tgt_list = [], []\n",
        "    for (_src, _tgt) in batch:\n",
        "        processed_src = torch.cat(\n",
        "            [\n",
        "                bs_id,\n",
        "                torch.tensor(\n",
        "                    src_vocab(src_tokenizer(_src)),\n",
        "                    dtype=torch.int64,\n",
        "                    device=device,\n",
        "                ),\n",
        "                eos_id,\n",
        "            ],\n",
        "            0,\n",
        "        )\n",
        "        processed_tgt = torch.cat(\n",
        "            [\n",
        "                bs_id,\n",
        "                torch.tensor(\n",
        "                    tgt_vocab(tgt_tokenizer(_tgt)),\n",
        "                    dtype=torch.int64,\n",
        "                    device=device,\n",
        "                ),\n",
        "                eos_id,\n",
        "            ],\n",
        "            0,\n",
        "        )\n",
        "        src_list.append(\n",
        "            # warning - overwrites values for negative values of padding - len\n",
        "            pad(\n",
        "                processed_src,\n",
        "                (\n",
        "                    0,\n",
        "                    max_padding - len(processed_src),\n",
        "                ),\n",
        "                value=pad_id,\n",
        "            )\n",
        "        )\n",
        "        tgt_list.append(\n",
        "            pad(\n",
        "                processed_tgt,\n",
        "                (0, max_padding - len(processed_tgt)),\n",
        "                value=pad_id,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    src = torch.stack(src_list)\n",
        "    tgt = torch.stack(tgt_list)\n",
        "    return (src, tgt)"
      ],
      "metadata": {
        "id": "YsIMjycO-3zO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return collate_batch(\n",
        "        batch,\n",
        "        tokenize_de,\n",
        "        tokenize_en,\n",
        "        vocab_src,\n",
        "        vocab_tgt,\n",
        "        device,\n",
        "        max_padding=max_padding,\n",
        "        pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "CrkstY7I_v4p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=12000\n",
        "max_padding=128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_map,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    val_map,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "gUOpzZczAoik"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}